{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "mount_file_id": "1bbnvNrXj_NRtvbVnHiXWp0hOv-SGUxfz",
      "authorship_tag": "ABX9TyP//5MBM7WA/nvaaBaZxCNJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denniswillie/FiQA_GPT2/blob/main/FiQA_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maF6sKbKcQ-M",
        "outputId": "8e377827-506f-411d-93a2-efd6f3fee079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoConfig, get_polynomial_decay_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LoadDataArgs():\n",
        "  def __init__(self):\n",
        "    self.data_dir=\"data\"\n",
        "    self.train_prefix=\"train\"\n",
        "    self.valid_prefix=\"valid\"\n",
        "    self.train_frac=0.85\n",
        "    self.model_type=\"gpt2\"\n",
        "\n",
        "class TrainArgs():\n",
        "  def __init__(self):\n",
        "    self.seed = 0\n",
        "    self.mode = \"train\"\n",
        "    self.data_dir = \"data\"\n",
        "    self.train_prefix=\"train\"\n",
        "    self.valid_prefix=\"valid\"\n",
        "    self.model_type=\"gpt2\"\n",
        "    self.bos_token=\"<bos>\"\n",
        "    self.sp1_token=\"<question>\"\n",
        "    self.sp2_token=\"<answer>\"\n",
        "    self.gpu=\"0\"\n",
        "    self.lr=2e-5\n",
        "    self.warmup_ratio = 0.0\n",
        "    self.batch_size = 2\n",
        "    self.num_workers = 0\n",
        "    self.num_epochs = 3\n",
        "    self.max_len = 1024\n",
        "    self.max_turns = 1\n",
        "    self.ckpt_dir = \"saved_models\"\n",
        "    self.ckpt_name = None\n",
        "\n",
        "class InferArgs():\n",
        "  def __init__(self):\n",
        "    self.seed = 0\n",
        "    self.mode = \"infer\"\n",
        "    self.data_dir = \"data\"\n",
        "    self.model_type = \"gpt2\"\n",
        "    self.bos_token = \"<bos>\"\n",
        "    self.sp1_token = \"<question>\"\n",
        "    self.sp2_token = \"<answer>\"\n",
        "    self.gpu = \"0\"\n",
        "    self.max_len = 1024\n",
        "    self.max_turns = 1\n",
        "    self.top_p = 0.8\n",
        "    self.ckpt_dir = \"saved_models\"\n",
        "    self.ckpt_name = \"something\"\n",
        "    self.end_command=\"Abort!\""
      ],
      "metadata": {
        "id": "CnVRYs98fHB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n60r_fDKkX--",
        "outputId": "8f56af75-5d21-4c82-cb9c-54289efde783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.9.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from datasets import *\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def save_data(prefix, data_dir, dialogues, tokenizer):\n",
        "    print(f\"Saving {prefix} text file...\")\n",
        "    with open(f\"{data_dir}/{prefix}_utters.json\", 'w') as f:\n",
        "        json.dump(dialogues, f)\n",
        "\n",
        "    print(f\"Saving {prefix} idx file...\")\n",
        "    ids = []\n",
        "    for dialogue in tqdm(dialogues):\n",
        "        dialogue_ids = []\n",
        "        for utter in dialogue:\n",
        "            tokens = tokenizer.tokenize(utter)\n",
        "            token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "            dialogue_ids.append(token_ids)\n",
        "        ids.append(dialogue_ids)\n",
        "\n",
        "    assert len(ids) == len(dialogues)\n",
        "\n",
        "    with open(f\"{data_dir}/{prefix}_ids.json\", 'w') as f:\n",
        "        json.dump(ids, f)\n",
        "\n",
        "from tqdm import tqdm\n",
        "from datasets import *\n",
        "\n",
        "\n",
        "# For all\n",
        "space = 'Ġ'\n",
        "pre_quote = '’'\n",
        "end_marks = ['.', ',', '?', '!', '...']\n",
        "quotes = ['\"', '\\'']\n",
        "abbreviations = ['s', 'd', 't', 'm', 're', 'll', 've', 'S', 'D', 'T', 'M', 'Re', 'Ll', 'Ve']\n",
        "\n",
        "def load_fiqa_dataset(tokenizer, train_frac):\n",
        "    # Load the FIQA dataset.\n",
        "    DATASET_DIRNAME = \"./drive/MyDrive/FiQA_train_task2/\"\n",
        "    QUESTIONS_FILENAME = \"FiQA_train_question_final.tsv\"\n",
        "    ANSWERS_FILENAME = \"FiQA_train_doc_final.tsv\"\n",
        "    QNA_PAIRS_FILENAME = \"FiQA_train_question_doc_final.tsv\"\n",
        "\n",
        "    q_df = pd.read_csv(DATASET_DIRNAME + QUESTIONS_FILENAME, delimiter='\\t')\n",
        "    a_df = pd.read_csv(DATASET_DIRNAME + ANSWERS_FILENAME, delimiter='\\t')\n",
        "    qna_df = pd.read_csv(DATASET_DIRNAME + QNA_PAIRS_FILENAME, delimiter='\\t')\n",
        "\n",
        "    # Merge dataframes based on the qna pairs\n",
        "    merged_df1 = qna_df.merge(q_df, left_on='qid', right_on='qid')\n",
        "    df = merged_df1.merge(a_df, left_on='docid', right_on='docid')\n",
        "    df.drop(['Unnamed: 0', 'Unnamed: 0_x','Unnamed: 0_y', 'qid', 'docid', 'timestamp_x', 'timestamp_y'], axis=1, inplace=True)\n",
        "    nan_rows = df[df.isnull().any(axis=1)]\n",
        "    before_drop = df.shape[0]\n",
        "    dropped_rows = df[df.isnull().any(axis=1)]\n",
        "    df = df.dropna(how='any')\n",
        "    df_dict = df.to_dict()\n",
        "\n",
        "    total_dialogues = []\n",
        "    for q, a in list(zip(df_dict['question'].values(), df_dict['doc'].values())):\n",
        "        total_dialogues.append([q, a])\n",
        "\n",
        "    for i, dialogue in enumerate(tqdm(total_dialogues)):\n",
        "        new_dialogue = []\n",
        "        for utter in dialogue:\n",
        "            token_list = tokenizer.tokenize(utter.strip().replace(pre_quote, quotes[1]))\n",
        "            token_list = process_token_list(token_list)\n",
        "            text = tokenizer.convert_tokens_to_string(token_list)\n",
        "            new_dialogue.append(text)\n",
        "\n",
        "        total_dialogues[i] = new_dialogue\n",
        "\n",
        "    train_utter_num = 0\n",
        "    valid_utter_num = 0\n",
        "    train_dialogues = total_dialogues[:int(len(total_dialogues)*train_frac)]\n",
        "    valid_dialogues = total_dialogues[int(len(total_dialogues)*train_frac):]\n",
        "\n",
        "    for dialogue in train_dialogues:\n",
        "        train_utter_num += len(dialogue)\n",
        "\n",
        "    for dialogue in valid_dialogues:\n",
        "        valid_utter_num += len(dialogue)\n",
        "\n",
        "    return train_dialogues, valid_dialogues, train_utter_num, valid_utter_num\n",
        "\n",
        "def process_token_list(token_list):\n",
        "    token_list[0] = token_list[0].capitalize()\n",
        "\n",
        "    quote_count = 0\n",
        "    for i, token in enumerate(token_list):\n",
        "        if space in token:\n",
        "            if token[1:] in end_marks or token[1:] in abbreviations:\n",
        "                token_list[i] = token[1:]\n",
        "\n",
        "            if token[1:] == quotes[1]:\n",
        "                if i<len(token_list)-1:\n",
        "                    if token_list[i+1] in abbreviations or (token_list[i+1][0] == space and token_list[i+1][1:] in abbreviations):\n",
        "                        token_list[i] = token[1:]\n",
        "\n",
        "        if len(token) > 1 and token[0] == space and token[1:] in quotes:\n",
        "            if quote_count % 2 == 1:\n",
        "                token_list[i] = token[1:]\n",
        "                quote_count = 0\n",
        "            else:\n",
        "                if i<len(token_list)-1 and token_list[i+1][0] == space:\n",
        "                    token_list[i+1] = token_list[i+1][1:]\n",
        "                quote_count += 1\n",
        "\n",
        "        if token in end_marks or token[1:] in end_marks:\n",
        "            if i<len(token_list)-1:\n",
        "                if token_list[i+1][0] != space:\n",
        "                    token_list[i+1] = space + token_list[i+1].capitalize()\n",
        "                else:\n",
        "                    token_list[i+1] = space + token_list[i+1][1:].capitalize()\n",
        "\n",
        "    new_token_list = [token for token in token_list if token != space and len(token)>0]\n",
        "    if new_token_list[-1] not in end_marks:\n",
        "        new_token_list.append(end_marks[0])\n",
        "\n",
        "    return new_token_list"
      ],
      "metadata": {
        "id": "4udrpAc7g_Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "import os\n",
        "args = LoadDataArgs()\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(args.model_type)\n",
        "args.data_dir = f\"{args.data_dir}/{args.model_type}\"\n",
        "if not os.path.isdir(args.data_dir):\n",
        "  os.makedirs(args.data_dir)\n",
        "\n",
        "print(\"Loading & Merging all datasets...\")\n",
        "train_dialogues, valid_dialogues, num_train, num_valid = load_fiqa_dataset(tokenizer, args.train_frac)\n",
        "\n",
        "print(\"Saving train data...\")\n",
        "save_data(args.train_prefix, args.data_dir, train_dialogues, tokenizer)\n",
        "print(\"Saving validation data...\")\n",
        "save_data(args.valid_prefix, args.data_dir, valid_dialogues, tokenizer)\n",
        "print(\"#\"*50 + \"Analysis on total data\" + \"#\"*50)\n",
        "print(f\"The number of train dialogues: {len(train_dialogues)}\")\n",
        "print(f\"The number of valid dialogues: {len(valid_dialogues)}\")\n",
        "print(f\"The number of train utterances: {num_train}\")\n",
        "print(f\"The number of valid utterances: {num_valid}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zmf5DN1OgXTJ",
        "outputId": "a7811bf4-b4ad-4297-9d00-0cfcb7cf5a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading & Merging all datasets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17072/17072 [00:30<00:00, 564.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train data...\n",
            "Saving train text file...\n",
            "Saving train idx file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14511/14511 [00:20<00:00, 698.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving validation data...\n",
            "Saving valid text file...\n",
            "Saving valid idx file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2561/2561 [00:03<00:00, 737.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##################################################Analysis on total data##################################################\n",
            "The number of train dialogues: 14511\n",
            "The number of valid dialogues: 2561\n",
            "The number of train utterances: 29022\n",
            "The number of valid utterances: 5122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from itertools import chain\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "import json\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, prefix, args):\n",
        "        assert prefix == args.train_prefix or prefix == args.valid_prefix\n",
        "\n",
        "        print(f\"Loading {prefix}_id.json...\")\n",
        "        with open(f\"{args.data_dir}/{prefix}_ids.json\", 'r') as f:\n",
        "            dials = json.load(f)\n",
        "\n",
        "        self.input_ids = []  # (N, L)\n",
        "        self.token_type_ids = []  # (N, L)\n",
        "        self.labels = []  # (N, L)\n",
        "\n",
        "        print(f\"Processing {prefix} data...\")\n",
        "\n",
        "        # dial = 1 dialog = multiple turns.\n",
        "        for dial in tqdm(dials):\n",
        "            hists = []\n",
        "            for u, utter in enumerate(dial):\n",
        "                if u % 2 == 0:\n",
        "                    hists.append([args.sp1_id] + utter)\n",
        "                else:\n",
        "                    hists.append([args.sp2_id] + utter)\n",
        "\n",
        "            assert len(hists) == 2\n",
        "\n",
        "            input_ids = [args.bos_id] + list(chain.from_iterable(hists)) + [args.eos_id]\n",
        "            if len(input_ids) <= args.max_len:\n",
        "                token_type_ids = [args.sp1_id] * len(hists[0]) + [args.sp2_id] * len(hists[1])\n",
        "                token_type_ids = [args.sp1_id] + token_type_ids + [args.sp2_id]\n",
        "                assert len(input_ids) == len(token_type_ids)\n",
        "\n",
        "                labels = []\n",
        "                for i, token_type_id in enumerate(token_type_ids):\n",
        "                    if token_type_id == args.sp1_id:\n",
        "                        labels.append(-100)\n",
        "                    elif token_type_id == args.sp2_id and token_type_ids[i - 1] == args.sp1_id:\n",
        "                        labels.append(-100)\n",
        "                    else:\n",
        "                        labels.append(input_ids[i])\n",
        "                assert len(input_ids) == len(labels)\n",
        "                self.input_ids.append(input_ids)\n",
        "                self.token_type_ids.append(token_type_ids)\n",
        "                self.labels.append(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.token_type_ids[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "class PadCollate():\n",
        "    def __init__(self, eos_id):\n",
        "        self.eos_id = eos_id\n",
        "\n",
        "    def pad_collate(self, batch):\n",
        "        input_ids, token_type_ids, labels =[], [], []\n",
        "        for idx, seqs in enumerate(batch):\n",
        "            input_ids.append(torch.LongTensor(seqs[0]))\n",
        "            token_type_ids.append(torch.LongTensor(seqs[1]))\n",
        "            labels.append(torch.LongTensor(seqs[2]))\n",
        "\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=self.eos_id)\n",
        "        token_type_ids = torch.nn.utils.rnn.pad_sequence(token_type_ids, batch_first=True, padding_value=self.eos_id)\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
        "\n",
        "        return input_ids, token_type_ids, labels\n"
      ],
      "metadata": {
        "id": "3RpvPVTCkQ-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_polynomial_decay_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from itertools import chain\n",
        "\n",
        "import torch\n",
        "import os, sys\n",
        "import numpy as np\n",
        "import argparse\n",
        "import copy\n",
        "import math\n",
        "import random\n",
        "\n",
        "\n",
        "class Manager():\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.args.device = torch.device(f\"cuda:{self.args.gpu}\")\n",
        "        else:\n",
        "            self.args.device = torch.device(\"mps\")\n",
        "\n",
        "        # Tokenizer & Vocab\n",
        "        print(\"Loading the tokenizer...\")\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.args.model_type)\n",
        "        special_tokens = {\n",
        "            'bos_token': self.args.bos_token,\n",
        "            'additional_special_tokens': [self.args.sp1_token, self.args.sp2_token]\n",
        "        }\n",
        "        self.args.eos_token = self.tokenizer.eos_token\n",
        "        num_new_tokens = self.tokenizer.add_special_tokens(special_tokens)\n",
        "        vocab = self.tokenizer.get_vocab()\n",
        "        self.args.vocab_size = len(vocab)\n",
        "        self.args.bos_id = vocab[self.args.bos_token]\n",
        "        self.args.eos_id = vocab[self.args.eos_token]\n",
        "        self.args.sp1_id = vocab[self.args.sp1_token]\n",
        "        self.args.sp2_id = vocab[self.args.sp2_token]\n",
        "\n",
        "        # Load model\n",
        "        print(\"Loading the model...\")\n",
        "        self.fix_seed(self.args.seed)\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(self.args.model_type).to(self.args.device)\n",
        "        self.model.resize_token_embeddings(self.args.vocab_size)\n",
        "\n",
        "        self.args.max_len = min(self.args.max_len, self.model.config.n_ctx)\n",
        "\n",
        "        if self.args.mode == 'train':\n",
        "            # Load optimizer\n",
        "            print(\"Loading the optimizer...\")\n",
        "            self.optim = torch.optim.AdamW(self.model.parameters(), lr=self.args.lr)\n",
        "            self.best_loss = sys.float_info.max\n",
        "            self.last_epoch = 0\n",
        "\n",
        "            # Load train & valid dataset\n",
        "            print(\"Loading train & valid data...\")\n",
        "            train_set = CustomDataset(self.args.train_prefix, self.args)\n",
        "            valid_set = CustomDataset(self.args.valid_prefix, self.args)\n",
        "            ppd = PadCollate(eos_id=self.args.eos_id)\n",
        "\n",
        "            self.train_loader = DataLoader(train_set,\n",
        "                                           collate_fn=ppd.pad_collate,\n",
        "                                           shuffle=True,\n",
        "                                           batch_size=self.args.batch_size,\n",
        "                                           num_workers=self.args.num_workers,\n",
        "                                           pin_memory=True)\n",
        "            self.valid_loader = DataLoader(valid_set,\n",
        "                                           collate_fn=ppd.pad_collate,\n",
        "                                           batch_size=self.args.batch_size,\n",
        "                                           num_workers=self.args.num_workers,\n",
        "                                           pin_memory=True)\n",
        "\n",
        "            if not os.path.exists(self.args.ckpt_dir):\n",
        "                os.makedirs(self.args.ckpt_dir)\n",
        "\n",
        "            # Calculate total training steps\n",
        "            num_batches = len(self.train_loader)\n",
        "            args.total_train_steps = args.num_epochs * num_batches\n",
        "            args.warmup_steps = int(args.warmup_ratio * args.total_train_steps)\n",
        "\n",
        "            self.sched = get_polynomial_decay_schedule_with_warmup(\n",
        "                self.optim,\n",
        "                num_warmup_steps=args.warmup_steps,\n",
        "                num_training_steps=args.total_train_steps,\n",
        "                power=2\n",
        "            )\n",
        "\n",
        "            self.writer = SummaryWriter()\n",
        "\n",
        "        if self.args.ckpt_name is not None:\n",
        "            ckpt_path = f\"{self.args.ckpt_dir}/{self.args.ckpt_name}.ckpt\"\n",
        "            if os.path.exists(ckpt_path):\n",
        "                print(\"Loading the trained checkpoint...\")\n",
        "                ckpt = torch.load(ckpt_path, map_location=self.args.device)\n",
        "                self.model.load_state_dict(ckpt['model_state_dict'])\n",
        "\n",
        "                if self.args.mode == 'train':\n",
        "                    print(f\"The training restarts with the specified checkpoint: {self.args.ckpt_name}.ckpt.\")\n",
        "                    self.optim.load_state_dict(ckpt['optim_state_dict'])\n",
        "                    self.sched.load_state_dict(ckpt['sched_state_dict'])\n",
        "                    self.best_loss = ckpt['loss']\n",
        "                    self.last_epoch = ckpt['epoch']\n",
        "                else:\n",
        "                    print(\"The inference will start with the specified checkpoint.\")\n",
        "            else:\n",
        "                print(f\"Cannot fine the specified checkpoint {ckpt_path}.\")\n",
        "                if self.args.mode == 'train':\n",
        "                    print(\"Training will start with the initialized model.\")\n",
        "                else:\n",
        "                    print(\"Cannot inference.\")\n",
        "                    exit()\n",
        "\n",
        "        print(\"Setting finished.\")\n",
        "\n",
        "    def train(self):\n",
        "        self.fix_seed(self.args.seed)  # Fix seed before training\n",
        "        print(\"Training starts.\")\n",
        "\n",
        "        start_epoch = self.last_epoch+1\n",
        "        for epoch in range(start_epoch, start_epoch+self.args.num_epochs):\n",
        "            self.model.train()\n",
        "\n",
        "            print(f\"#\"*50 + f\"Epoch: {epoch}\" + \"#\"*50)\n",
        "            train_losses = []\n",
        "            train_ppls = []\n",
        "            for i, batch in enumerate(tqdm(self.train_loader)):\n",
        "                input_ids, token_type_ids, labels = batch\n",
        "                input_ids, token_type_ids, labels = \\\n",
        "                    input_ids.to(self.args.device), token_type_ids.to(self.args.device), labels.to(self.args.device)\n",
        "\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    token_type_ids = token_type_ids,\n",
        "                    labels = labels\n",
        "                )\n",
        "\n",
        "                loss, logits = outputs[0], outputs[1]\n",
        "\n",
        "                self.optim.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optim.step()\n",
        "                self.sched.step()\n",
        "\n",
        "                train_losses.append(loss.detach())\n",
        "                ppl = torch.exp(loss.detach())\n",
        "                train_ppls.append(ppl)\n",
        "\n",
        "            train_losses = [loss.item() for loss in train_losses]\n",
        "            train_ppls = [ppl.item() if not math.isinf(ppl.item()) else 1e+8 for ppl in train_ppls]\n",
        "            train_loss = np.mean(train_losses)\n",
        "            train_ppl = np.mean(train_ppls)\n",
        "            print(f\"Train loss: {train_loss} || Train perplexity: {train_ppl}\")\n",
        "\n",
        "            self.writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
        "            self.writer.add_scalar(\"PPL/train\", train_ppl, epoch)\n",
        "\n",
        "            self.last_epoch += 1\n",
        "\n",
        "            valid_loss, valid_ppl = self.validation()\n",
        "\n",
        "            if valid_loss < self.best_loss:\n",
        "                self.best_loss = valid_loss\n",
        "                state_dict = {\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optim_state_dict': self.optim.state_dict(),\n",
        "                    'sched_state_dict': self.sched.state_dict(),\n",
        "                    'loss': self.best_loss,\n",
        "                    'epoch': self.last_epoch\n",
        "                }\n",
        "\n",
        "                torch.save(state_dict, f\"{self.args.ckpt_dir}/best_ckpt_epoch={epoch}_valid_loss={round(self.best_loss, 4)}.ckpt\")\n",
        "                print(\"*\"*10 + \"Current best checkpoint is saved.\" + \"*\"*10)\n",
        "                print(f\"{self.args.ckpt_dir}/best_ckpt_epoch={epoch}_valid_loss={round(self.best_loss, 4)}.ckpt\")\n",
        "\n",
        "            print(f\"Best valid loss: {self.best_loss}\")\n",
        "            print(f\"Valid loss: {valid_loss} || Valid perplexity: {valid_ppl}\")\n",
        "\n",
        "            self.writer.add_scalar(\"Loss/valid\", valid_loss, epoch)\n",
        "            self.writer.add_scalar(\"PPL/valid\", valid_ppl, epoch)\n",
        "\n",
        "            self.writer.add_scalars(\"Losses\", {\n",
        "                'train': train_loss,\n",
        "                'valid': valid_loss,\n",
        "            }, epoch)\n",
        "            self.writer.add_scalars(\"PPLs\", {\n",
        "                'train': train_ppl,\n",
        "                'valid': valid_ppl,\n",
        "            }, epoch)\n",
        "\n",
        "        print(\"Training finished!\")\n",
        "\n",
        "    def validation(self):\n",
        "        print(\"Validation processing...\")\n",
        "        self.model.eval()\n",
        "\n",
        "        valid_losses = []\n",
        "        valid_ppls = []\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(tqdm(self.valid_loader)):\n",
        "                input_ids, token_type_ids, labels = batch\n",
        "                input_ids, token_type_ids, labels = \\\n",
        "                    input_ids.to(self.args.device), token_type_ids.to(self.args.device), labels.to(self.args.device)\n",
        "\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    token_type_ids = token_type_ids,\n",
        "                    labels = labels\n",
        "                )\n",
        "\n",
        "                loss, logits = outputs[0], outputs[1]\n",
        "\n",
        "                valid_losses.append(loss.detach())\n",
        "                ppl = torch.exp(loss.detach())\n",
        "                valid_ppls.append(ppl)\n",
        "\n",
        "            valid_losses = [loss.item() for loss in valid_losses]\n",
        "            valid_ppls = [ppl.item() if not math.isinf(ppl.item()) else 1e+8 for ppl in valid_ppls]\n",
        "            valid_loss = np.mean(valid_losses)\n",
        "            valid_ppl = np.mean(valid_ppls)\n",
        "\n",
        "            if math.isnan(valid_ppl):\n",
        "                valid_ppl = 1e+8\n",
        "\n",
        "        return valid_loss, valid_ppl\n",
        "\n",
        "\n",
        "    def infer(self):\n",
        "        print(\"Let's start!\")\n",
        "        print(f\"If you want to quit the conversation, please type \\\"{self.args.end_command}\\\".\")\n",
        "        self.model.eval()\n",
        "        self.fix_seed(self.args.seed)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            question = input(\"Question: \")\n",
        "            input_ids = [self.args.sp1_id] + self.tokenizer.encode(question)\n",
        "            input_ids = [self.args.bos_id] + input_ids + [self.args.sp2_id]\n",
        "            token_type_ids = [self.args.sp1_id] * (len(input_ids) - 1) + [self.args.sp2_id]\n",
        "            assert len(input_ids) == len(token_type_ids)\n",
        "            input_len = len(input_ids)\n",
        "\n",
        "            input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(self.args.device)\n",
        "            token_type_ids = torch.LongTensor(token_type_ids).unsqueeze(0).to(self.args.device)\n",
        "\n",
        "            output_ids = self.nucleus_sampling(input_ids, token_type_ids, input_len)\n",
        "            res = self.tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "            print(\"Answer: {}\".format(res))\n",
        "\n",
        "    def nucleus_sampling(self, input_ids, token_type_ids, input_len):\n",
        "        output_ids = []\n",
        "        for pos in range(input_len, self.args.max_len):\n",
        "            output = self.model(input_ids=input_ids, token_type_ids=token_type_ids)  # (1, V)\n",
        "            output = output[0][:, pos-1]\n",
        "            output = F.softmax(output, dim=-1)  # (1, V)\n",
        "\n",
        "            sorted_probs, sorted_idxs = torch.sort(output, descending=True)\n",
        "            cumsum_probs = torch.cumsum(sorted_probs, dim=-1)  # (1, V)\n",
        "            idx_remove = cumsum_probs > self.args.top_p\n",
        "            idx_remove[:, 1:] = idx_remove[:, :-1].clone()\n",
        "            idx_remove[:, 0] = False\n",
        "            sorted_probs[idx_remove] = 0.0\n",
        "            sorted_probs /= torch.sum(sorted_probs, dim=-1, keepdim=True)  # (1, V)\n",
        "\n",
        "            probs = torch.zeros(output.shape, device=self.args.device).scatter_(-1, sorted_idxs, sorted_probs)  # (1, V)\n",
        "            idx = torch.multinomial(probs, 1)  # (1, 1)\n",
        "\n",
        "            idx_item = idx.squeeze(-1).squeeze(-1).item()\n",
        "            output_ids.append(idx_item)\n",
        "\n",
        "            if idx_item == self.args.eos_id:\n",
        "                break\n",
        "\n",
        "            input_ids = torch.cat((input_ids, idx), dim=-1)\n",
        "            next_type_id = torch.LongTensor([[self.args.sp2_id]]).to(self.args.device)\n",
        "            token_type_ids = torch.cat((token_type_ids, next_type_id), dim=-1)\n",
        "            assert input_ids.shape == token_type_ids.shape\n",
        "\n",
        "            break\n",
        "\n",
        "        return output_ids\n",
        "\n",
        "    def fix_seed(self, seed):\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        random.seed(seed)"
      ],
      "metadata": {
        "id": "lE7-kLFxkpSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainArgs()\n",
        "args.data_dir = f\"{args.data_dir}/{args.model_type}\"\n",
        "args.ckpt_dir = f\"{args.ckpt_dir}/{args.model_type}\"\n",
        "manager = Manager(args)\n",
        "manager.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "wdmemhsokrL_",
        "outputId": "3d095dd6-4852-4b61-bcd2-f70afbc1d6d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the tokenizer...\n",
            "Loading the model...\n",
            "Loading the optimizer...\n",
            "Loading train & valid data...\n",
            "Loading train_id.json...\n",
            "Processing train data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14511/14511 [00:01<00:00, 9396.90it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading valid_id.json...\n",
            "Processing valid data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 86%|████████▌ | 2202/2561 [00:00<00:00, 11641.12it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-2a631ee6669f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{args.data_dir}/{args.model_type}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{args.ckpt_dir}/{args.model_type}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmanager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-9d88ac9b8e5f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading train & valid data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mvalid_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mppd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPadCollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meos_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-66c0d9bd0dbe>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, prefix, args)\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtoken_type_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp1_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                     \u001b[0;32melif\u001b[0m \u001b[0mtoken_type_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp2_id\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp1_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = InferArgs()\n",
        "args.ckpt_name = \"gpt2/best_ckpt_epoch=3_valid_loss=3.258\"\n",
        "manager = Manager(args)\n",
        "manager.infer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL4hpWBD0kfJ",
        "outputId": "eea77e86-04d3-4e25-c472-bfabc3afc43d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the tokenizer...\n",
            "Loading the model...\n",
            "Loading the trained checkpoint...\n",
            "The inference will start with the specified checkpoint.\n",
            "Setting finished.\n",
            "Let's start!\n",
            "If you want to quit the conversation, please type \"Abort!\".\n",
            "Question: Should i have a business credit card?\n",
            "tensor([[1532]], device='cuda:0')\n",
            "1532\n",
            "Answer: If\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FBYV5NQnM8fX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}